{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b949f9f",
   "metadata": {},
   "source": [
    "# Project Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbcb066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "import sys\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from models.blip import blip_decoder\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a811a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demo_image(img_path, image_size, device):\n",
    "    raw_image = Image.open(img_path).convert('RGB')   \n",
    "\n",
    "    w,h = raw_image.size\n",
    "    display(raw_image.resize((w//2,h//2)))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f4406",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "Perform image captioning of UI screenshots using BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3d32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/501.jpg'\n",
    "image_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7506b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the finetuned models and baseline BLIP models\n",
    "paths = [\n",
    "    '/Users/michaelmbajwa/Downloads/model_base_14M.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-B/BLIP_14M_25_EPOCHS/output/Caption_coco/checkpoint_best.pth',\n",
    "    '/Users/michaelmbajwa/Downloads/model_base.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-B/BLIP_25_EPOCHS/output/Caption_coco/checkpoint_best.pth',\n",
    "    '/Users/michaelmbajwa/Downloads/model_base_capfilt_large.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-B_CapFilt-L/BLIP_25_EPOCH/output/Caption_coco/checkpoint_best.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-B_CapFilt-L/BLIP_100_EPOCHS/output/Caption_coco/checkpoint_best.pth',\n",
    "    '/Users/michaelmbajwa/Downloads/model_large.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-L/BLIP_25_EPOCHS/output/Caption_coco/checkpoint_best.pth',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/BLIP_w:_ViT-L/BLIP_100_EPOCHS/output/Caption_coco/checkpoint_best.pth'\n",
    "]\n",
    "\n",
    "# Paths to images for testing\n",
    "images = [\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/501.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/505.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/15697.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/17832.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/71268.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/72164.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/72201.jpg',\n",
    "    '/Users/michaelmbajwa/Desktop/DL_Project/Image_Testing/IMG_D89B530EB834-1.jpeg',\n",
    "    '/Users/michaelmbajwa/Downloads/PHOTO-2022-06-01-15-05-54.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a730fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = defaultdict()\n",
    "for img_path in images:\n",
    "    output2 = defaultdict()\n",
    "    for model_path in paths:\n",
    "        image = load_demo_image(img_path=img_path, image_size=384, device=device)\n",
    "\n",
    "        if 'ViT-L' in model_path or model_path=='/Users/michaelmbajwa/Downloads/model_large.pth':\n",
    "            model = blip_decoder(pretrained=model_path, image_size=384, vit='large')\n",
    "        else:\n",
    "            model = blip_decoder(pretrained=model_path, image_size=384, vit='base')\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # beam search\n",
    "            caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
    "        \n",
    "        output2[model_path] = caption\n",
    "    output[img_path] = output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7378e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/michaelmbajwa/Downloads/results.json\", \"w\") as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
